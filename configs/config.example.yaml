app:
  name: trafficpulse
  timezone: Asia/Taipei

paths:
  raw_dir: data/raw
  processed_dir: data/processed
  cache_dir: data/cache
  outputs_dir: outputs

warehouse:
  # Embedded analytics-friendly storage (no external services): Parquet files + DuckDB query engine.
  enabled: false
  parquet_dir: data/processed/parquet
  duckdb_path: data/processed/trafficpulse.duckdb
  use_duckdb: true

cache:
  enabled: true
  ttl_seconds: 3600

tdx:
  base_url: https://tdx.transportdata.tw/api/basic/v2
  base_url_v1: https://tdx.transportdata.tw/api/basic/v1
  historical_base_url: https://tdx.transportdata.tw/api/historical
  token_url: https://tdx.transportdata.tw/auth/realms/TDXConnect/protocol/openid-connect/token
  request_timeout_seconds: 30
  max_retries: 3
  # Base delay (seconds) used for exponential backoff when requests fail (timeouts/429/5xx).
  retry_backoff_seconds: 1.0
  # Backoff growth factor per attempt (e.g., 1.0, 2.0, 1.5).
  backoff_multiplier: 2.0
  # Cap the maximum sleep between retries so a single request does not stall for too long.
  max_backoff_seconds: 60.0
  # Add a small random jitter to avoid thundering herds when many requests retry together.
  jitter_seconds: 0.25
  # If TDX responds with Retry-After (429/503), honor it as the minimum delay before retrying.
  respect_retry_after: true
  # Optional client-side throttle (seconds between requests). Use when you consistently hit 429.
  # Example: 0.2 ~= 5 requests/second.
  min_request_interval_seconds: 0.0

ingestion:
  # MVP uses VD observations as the initial "segment" definition.
  dataset: vd
  # Chunk long time windows to avoid oversized requests.
  query_chunk_minutes: 60
  vd:
    # The client will try these endpoints in order.
    # Note: exact availability can vary by dataset; adjust as needed.
    endpoint_templates:
      - Road/Traffic/Live/VD/City/{city}
    # Optional historical backfill endpoint (JSONL by date range, up to 7 days per request).
    historical_endpoint_templates:
      - v2/Historical/Road/Traffic/Live/VD/City/{city}
    cities:
      - Taipei
    # Field names and normalization rules for the selected dataset.
    time_field: DataCollectTime
    segment_id_field: VDID
    lane_list_field: VDLives
    lane_speed_field: Speed
    lane_volume_field: Volume
    lane_occupancy_field: Occupancy
    lane_speed_aggregation: volume_weighted_mean # [volume_weighted_mean, mean]
    lane_volume_aggregation: sum
    lane_occupancy_aggregation: mean
    metadata_fields:
      name_field: RoadSection
      direction_field: Direction
      road_name_field: RoadName
      link_id_field: RoadID
      lat_field: PositionLat
      lon_field: PositionLon
    paging:
      page_size: 1000
  events:
    # Traffic events/incidents (field names can vary; keep this config as the source of truth).
    # The client will try these endpoints in order.
    endpoint_templates:
      - Traffic/RoadEvent/LiveEvent/City/{city}
    historical_endpoint_templates:
      - Traffic/RoadEvent/Event/City/{city}
    cities:
      - Taipei
    start_time_field: EffectiveTime
    end_time_field: ExpireTime
    id_field: EventID
    type_field: EventType
    description_field: Description
    road_name_field: Location
    direction_field: Direction
    severity_field: Severity
    # Lat/Lon fields can also be configured as dot-paths (e.g., "Position.PositionLat").
    lat_field: ""
    lon_field: ""
    paging:
      page_size: 1000

preprocessing:
  source_granularity_minutes: 5
  # Target aggregation interval used for downstream analytics and API responses.
  target_granularity_minutes: 15
  aggregation:
    speed_kph: mean
    volume: sum
    occupancy_pct: mean

analytics:
  reliability:
    congestion_speed_threshold_kph: 30
    min_samples: 12
    # Used by the API when /rankings/reliability is called without an explicit time range.
    default_window_hours: 24
    weights:
      mean_speed: 0.4
      speed_std: 0.3
      congestion_frequency: 0.3
  corridors:
    corridors_csv: configs/corridors.csv
    # How to combine segment speeds into a corridor speed time series:
    # - volume: volume-weighted mean per timestamp (fallback to mean if volume missing)
    # - equal: simple mean across segments per timestamp
    # - static: use per-segment weight from corridors_csv
    speed_weighting: volume
    weight_column: weight
  anomalies:
    # Explainable anomaly detection baseline (no ML): rolling z-score on speed.
    method: rolling_zscore
    window_points: 12
    z_threshold: 3.0
    direction: low # low | high | both
    max_gap_minutes: 30
    min_event_points: 2
  event_impact:
    # Baseline impact analysis using events/incidents + speed time series (no ML).
    default_window_hours: 24
    radius_meters: 1000
    max_segments: 50
    baseline_window_minutes: 60
    end_time_fallback_minutes: 60
    recovery_horizon_minutes: 180
    recovery_ratio: 0.9
    speed_weighting: volume # volume | equal
    min_baseline_points: 4
    min_event_points: 2

api:
  host: 0.0.0.0
  port: 8000
  cache:
    enabled: true
    ttl_seconds: 60
    include_paths:
      - /map/snapshot
      - /rankings/reliability
      - /rankings/reliability/corridors
      - /events
  rate_limit:
    enabled: true
    window_seconds: 60
    max_requests: 60
    include_paths:
      - /map/snapshot
      - /rankings/reliability
      - /rankings/reliability/corridors
      - /events
  cors:
    allow_origins:
      - http://localhost:8000
      - http://localhost:5173
